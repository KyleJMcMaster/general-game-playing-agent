{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5819fc30",
   "metadata": {},
   "source": [
    "### GPT for Learning how to play 2048\n",
    "\n",
    "Inspired by [GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c2d08",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee4548fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000201000000000000000000000000,00000000000000000000000000010201,00010201000000000000000000000001,00010202010000000000000000000000,01030000010000010000000000000000,02030001000000000000000000010000,02030001000100000000000001000000,02030001010100000000000000010000,02030001010200000000000000020000,02030001010300000000000001000000,02040001020000000000010000000000,03040101000000000000000000000100,03040201000000000000000000010000,03040201010100000000000000000000,03040201020000000002000000000000,03040201020200010000000000000000,03040202020200010000000000000000,03040300030100000001000000000000,04040300000200000000000000020000,00000000000000000004010004030300,00000000000000000000040101000404,00000000000000010000040100000105,00000000000000000001040200000105,00000000000000000001040200010105,00000000010000000001040200000205,00000100000000000000040201010205,00010001000000000000040200020205,00000002010000000000040200000305,00000100000000000000040301000305,00000000000001000100040301000305,0000010000\n",
      "4315376\n"
     ]
    }
   ],
   "source": [
    "with open('games.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:1000])\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f035a56",
   "metadata": {},
   "source": [
    "#### Tokenize\n",
    "\n",
    "Each tile is represented by 2 digit, 0-led int. Will convert tile into regular int. \n",
    "\n",
    "Each state is separated by comma, each game by ';\\n'. Will convert these into int as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c614918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 0, 0, 2, 3, 3, 1, 6, 7, 6, 5, 7, 8, 10, 11, 17, 1, 0, 0, 0, 3, 3, 3, 1, 6, 7, 6, 5, 7, 8, 10, 11, 17, 18, 1, 0, 0, 0, 3, 3, 3, 1, 6, 7, 6, 5, 7, 8, 10, 11, 17]\n",
      "02000000020303010607060507081011,01000000030303010607060507081011,;\n",
      "01000000030303010607060507081011,\n"
     ]
    }
   ],
   "source": [
    "mapping = {',':17, ';\\n':18}\n",
    "inv_mapping = {v: k for k, v in mapping.items()}\n",
    "\n",
    "def encode(s:str)-> list[str]:\n",
    "    out = []\n",
    "    num_splits = len(s.split(f';\\n'))\n",
    "    for i, game in enumerate(s.split(f';\\n')): \n",
    "        for state in game.split(',')[:-1]:\n",
    "            enc_state = [int(''.join(state[i:i+2])) for i in range(0, len(state), 2)]\n",
    "            enc_state.append(mapping[','])\n",
    "            out += enc_state\n",
    "        out.append(mapping[';\\n']) if i < num_splits-1 else None\n",
    "    \n",
    "    return out\n",
    "\n",
    "def decode(l:list[int]) -> str:\n",
    "    s = ''\n",
    "    for char in l:\n",
    "        if char < 10:\n",
    "            s += '0'+ str(char)\n",
    "        elif char < min(mapping.values()): # hard coded, should fix\n",
    "            s += str(char)\n",
    "        else:\n",
    "            s += inv_mapping[char]\n",
    "\n",
    "    return s        \n",
    "\n",
    "\n",
    "print(encode('02000000020303010607060507081011,01000000030303010607060507081011,;\\n01000000030303010607060507081011,'))\n",
    "print(decode(encode('02000000020303010607060507081011,01000000030303010607060507081011,;\\n01000000030303010607060507081011,')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41638313",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "\n",
    "Get train test split\n",
    "\n",
    "Set up batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67652d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "tensor([[17,  1,  0,  1,  0,  3,  1,  0,  0,  4,  6,  3,  0,  8,  1,  9, 11, 17,\n",
      "          1,  0,  0,  0,  3,  1,  1,  0,  4,  6,  3,  1,  8,  1,  9, 11],\n",
      "        [ 3,  7,  8, 10, 17,  0,  1,  0,  0,  0,  1,  0,  1,  3,  5,  4,  2,  3,\n",
      "          7,  8, 10, 17,  0,  0,  0,  1,  0,  1,  0,  2,  3,  5,  4,  2],\n",
      "        [ 6,  4,  3,  0,  2,  5,  3,  1,  3,  2,  1,  0, 17,  1,  3,  2,  3,  6,\n",
      "          4,  4,  1,  2,  5,  1,  1,  3,  2,  0,  0, 17,  1,  3,  2,  3],\n",
      "        [ 0,  1,  0,  2,  0,  0,  0,  4,  3,  0,  0,  5,  6,  8,  9, 17,  1,  0,\n",
      "          1,  0,  2,  0,  0,  0,  4,  3,  0,  0,  5,  6,  8,  9, 17,  1]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 1,  0,  1,  0,  3,  1,  0,  0,  4,  6,  3,  0,  8,  1,  9, 11, 17,  1,\n",
      "          0,  0,  0,  3,  1,  1,  0,  4,  6,  3,  1,  8,  1,  9, 11, 17],\n",
      "        [ 7,  8, 10, 17,  0,  1,  0,  0,  0,  1,  0,  1,  3,  5,  4,  2,  3,  7,\n",
      "          8, 10, 17,  0,  0,  0,  1,  0,  1,  0,  2,  3,  5,  4,  2,  3],\n",
      "        [ 4,  3,  0,  2,  5,  3,  1,  3,  2,  1,  0, 17,  1,  3,  2,  3,  6,  4,\n",
      "          4,  1,  2,  5,  1,  1,  3,  2,  0,  0, 17,  1,  3,  2,  3,  0],\n",
      "        [ 1,  0,  2,  0,  0,  0,  4,  3,  0,  0,  5,  6,  8,  9, 17,  1,  0,  1,\n",
      "          0,  2,  0,  0,  0,  4,  3,  0,  0,  5,  6,  8,  9, 17,  1,  0]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1748)\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 34 # two boards\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 100\n",
    "num_iters = 1000\n",
    "eval_interval = 100\n",
    "print(device)\n",
    "\n",
    "# get first game in last 20% of data\n",
    "n = int(0.8*len(text))\n",
    "while(text[n] != ';'):\n",
    "    n += 1\n",
    "n += 2\n",
    "\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]\n",
    "\n",
    "def get_batch(split:bool=0)-> list[torch.Tensor]:\n",
    "    # split == 0: train, 1: test\n",
    "    data = train_data if split == 0 else test_data\n",
    "    print(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch()\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d6662",
   "metadata": {},
   "source": [
    "#### Bigram Model\n",
    "\n",
    "Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54e74866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([136, 19])\n",
      "tensor(3.6508, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "000210,13130810081210100503030303080906071606091406120814030509141101071608121305120814020101050710040413050406150816061208101514;\n",
      "030308;\n",
      "14150813050306120709001510,0101,11,,13050612131510;\n",
      "161212\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1748)\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size=19):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T)\n",
    "        for  _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:,-1,:] # last time step, (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n",
    "        return idx\n",
    "\n",
    "    \n",
    "model = BigramModel()\n",
    "m = model.to(device)\n",
    "logits, loss = m(x,y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd87befe",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "random_ expects 'from' to be less than 'to', but got from=0 >= to=-34",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iters):\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m % eval_interval == \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m         losses = \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     25\u001b[39m     xb, yb = get_batch(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ggpa-env/lib/python3.13/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mestimate_loss\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m losses = torch.zeros(eval_iters)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     X,Y = \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     logits, loss = model(X,Y)\n\u001b[32m     10\u001b[39m     losses[k] = loss.item()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mget_batch\u001b[39m\u001b[34m(split)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_batch\u001b[39m(split:\u001b[38;5;28mbool\u001b[39m=\u001b[32m0\u001b[39m)-> \u001b[38;5;28mlist\u001b[39m[torch.Tensor]:\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m# split == 0: train, 1: test\u001b[39;00m\n\u001b[32m     26\u001b[39m     data = train_data \u001b[38;5;28;01mif\u001b[39;00m split == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m test_data\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     ix = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     x = torch.stack([data[i:i+block_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[32m     29\u001b[39m     y = torch.stack([data[i+\u001b[32m1\u001b[39m:i+block_size+\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n",
      "\u001b[31mRuntimeError\u001b[39m: random_ expects 'from' to be less than 'to', but got from=0 >= to=-34"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [0, 1]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "for iter in range(num_iters):\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'{iter}: Train loss: {losses[0]:.4f}, Val loss: {losses[1]:.4f}')\n",
    "    xb, yb = get_batch(0)\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n",
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=100)[0].tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16434495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000050811,010003000000030705020000010104060100000004030201000000000001020200021112,0000020103020302030200010101020307091011,0100010100,07060810,0607000000000000010104,02070811,00000410,000101\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ggpa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
