{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5819fc30",
   "metadata": {},
   "source": [
    "### GPT for Learning how to play 2048\n",
    "\n",
    "Inspired by [GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c2d08",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee4548fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000201000000000000000000000000,00000000000000000000000000010201,00010201000000000000000000000001,00010202010000000000000000000000,01030000010000010000000000000000,02030001000000000000000000010000,02030001000100000000000001000000,02030001010100000000000000010000,02030001010200000000000000020000,02030001010300000000000001000000,02040001020000000000010000000000,03040101000000000000000000000100,03040201000000000000000000010000,03040201010100000000000000000000,03040201020000000002000000000000,03040201020200010000000000000000,03040202020200010000000000000000,03040300030100000001000000000000,04040300000200000000000000020000,00000000000000000004010004030300,00000000000000000000040101000404,00000000000000010000040100000105,00000000000000000001040200000105,00000000000000000001040200010105,00000000010000000001040200000205,00000100000000000000040201010205,00010001000000000000040200020205,00000002010000000000040200000305,00000100000000000000040301000305,00000000000001000100040301000305,0000010000\n",
      "4315376\n"
     ]
    }
   ],
   "source": [
    "with open('games.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:1000])\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f035a56",
   "metadata": {},
   "source": [
    "#### Tokenize\n",
    "\n",
    "Each tile is represented by 2 digit, 0-led int. Will convert tile into regular int. \n",
    "\n",
    "Each state is separated by comma, each game by ';\\n'. Will convert these into int as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c614918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 0, 0, 0, 2, 3, 3, 1, 6, 7, 6, 5, 7, 8, 10, 11, 17, 1, 0, 0, 0, 3, 3, 3, 1, 6, 7, 6, 5, 7, 8, 10, 11, 17, 18, 1, 0, 0, 0, 3, 3, 3, 1, 6, 7, 6, 5, 7, 8, 10, 11, 17]\n",
      "02000000020303010607060507081011,01000000030303010607060507081011,;\n",
      "01000000030303010607060507081011,\n"
     ]
    }
   ],
   "source": [
    "mapping = {',':17, ';\\n':18}\n",
    "inv_mapping = {v: k for k, v in mapping.items()}\n",
    "\n",
    "def encode(s:str)-> list[str]:\n",
    "    out = []\n",
    "    num_splits = len(s.split(f';\\n'))\n",
    "    for i, game in enumerate(s.split(f';\\n')): \n",
    "        for state in game.split(',')[:-1]:\n",
    "            enc_state = [int(''.join(state[i:i+2])) for i in range(0, len(state), 2)]\n",
    "            enc_state.append(mapping[','])\n",
    "            out += enc_state\n",
    "        out.append(mapping[';\\n']) if i < num_splits-1 else None\n",
    "    \n",
    "    return out\n",
    "\n",
    "def decode(l:list[int]) -> str:\n",
    "    s = ''\n",
    "    for char in l:\n",
    "        if char < 10:\n",
    "            s += '0'+ str(char)\n",
    "        elif char < min(mapping.values()): # hard coded, should fix\n",
    "            s += str(char)\n",
    "        else:\n",
    "            s += inv_mapping[char]\n",
    "\n",
    "    return s        \n",
    "\n",
    "\n",
    "print(encode('02000000020303010607060507081011,01000000030303010607060507081011,;\\n01000000030303010607060507081011,'))\n",
    "print(decode(encode('02000000020303010607060507081011,01000000030303010607060507081011,;\\n01000000030303010607060507081011,')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41638313",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "\n",
    "Get train test split\n",
    "\n",
    "Set up batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f67652d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1816089 2223064 1816089 406975\n",
      "tensor([[ 0,  6, 17,  ...,  2,  1,  1],\n",
      "        [ 6,  1,  0,  ...,  1,  5,  3],\n",
      "        [ 2,  5,  5,  ...,  8,  4,  1],\n",
      "        ...,\n",
      "        [ 2,  6,  4,  ...,  1,  1,  2],\n",
      "        [ 5,  1,  0,  ...,  5,  2,  3],\n",
      "        [ 2,  2,  5,  ...,  0,  5,  1]], device='cuda:0')\n",
      "tensor([[ 6, 17,  0,  ...,  1,  1,  0],\n",
      "        [ 1,  0,  1,  ...,  5,  3,  3],\n",
      "        [ 5,  5,  2,  ...,  4,  1,  1],\n",
      "        ...,\n",
      "        [ 6,  4,  3,  ...,  1,  2,  2],\n",
      "        [ 1,  0,  0,  ...,  2,  3,  2],\n",
      "        [ 2,  5,  6,  ...,  5,  1,  0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1748)\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "batch_size = 64\n",
    "block_size = 34 * 3 # two boards * n\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "num_iters = 5000\n",
    "eval_iters = 200\n",
    "eval_interval = num_iters//10\n",
    "n_embed = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout= 0.2\n",
    "learning_rate = 3e-4\n",
    "print(device)\n",
    "\n",
    "# get first game in last 20% of data\n",
    "n = int(0.8*len(data))\n",
    "while(data[n] != mapping[';\\n']):\n",
    "    n += 1\n",
    "n += 2\n",
    "\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]\n",
    "print(n, len(data), len(train_data), len(test_data))\n",
    "\n",
    "def get_batch(split:bool=0)-> list[torch.Tensor]:\n",
    "    # split == 0: train, 1: test\n",
    "    data = train_data if split == 0 else test_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch()\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d6662",
   "metadata": {},
   "source": [
    "#### Bigram Model\n",
    "\n",
    "Loss With multi-self attention: 4900: Train loss: 1.2557, Val loss: 1.1633\n",
    "\n",
    "Residual Blocks: 4900: Train loss: 1.2156, Val loss: 1.1171\n",
    "Seems to have learned state size\n",
    "\n",
    "Scaling up model 4500: Train loss: 0.4378, Val loss: 0.4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54e74866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6528, 19])\n",
      "tensor(3.0377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "00041615020706000607110606030403070412131214060400090004,160811001100031308051306011111;\n",
      "03031512090100160602,0612131502100307;\n",
      "0211031506020808070200;\n",
      "020811001203,0811070016050903101209061313080515\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1748)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4* n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed//n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size=19):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B,T)\n",
    "        for  _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:] # last time step, (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) #(B, T+1)\n",
    "        return idx\n",
    "\n",
    "    \n",
    "model = BigramModel()\n",
    "m = model.to(device)\n",
    "logits, loss = m(x,y)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd87befe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Train loss: 3.0343, Val loss: 3.0404\n",
      "500: Train loss: 0.8938, Val loss: 0.8033\n",
      "1000: Train loss: 0.7611, Val loss: 0.6797\n",
      "1500: Train loss: 0.6412, Val loss: 0.5678\n",
      "2000: Train loss: 0.5742, Val loss: 0.5127\n",
      "2500: Train loss: 0.5253, Val loss: 0.4721\n",
      "3000: Train loss: 0.4925, Val loss: 0.4486\n",
      "3500: Train loss: 0.4712, Val loss: 0.4349\n",
      "4000: Train loss: 0.4528, Val loss: 0.4228\n",
      "4500: Train loss: 0.4378, Val loss: 0.4096\n",
      "0000000001050000020100,02020103000000000001000001020000,00020103000000000000010100000102,00010303000200000000000100000202,00000000000000000001030400020202,00000000010000000103040003000200,01000000\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [0, 1]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "for iter in range(num_iters):\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'{iter}: Train loss: {losses[0]:.4f}, Val loss: {losses[1]:.4f}')\n",
    "    xb, yb = get_batch(0)\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "res = decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=100)[0].tolist())\n",
    "print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16434495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000003010306040103071011,00010201000003010306040103071011,01020100030100010306040103071011,00000001010101000306040204071011,00010001000000020306040204071011,00000001000100020306040204071011,010001\n",
      "[24, 32, 32, 32, 32, 32, 6]\n"
     ]
    }
   ],
   "source": [
    "res = decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=100)[0].tolist())\n",
    "print(res)\n",
    "print([len(state) for state in res.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9991c5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000000202040204070910,00000001000100000202040204070910,00010000000100010202040204070910,00000000000201010202040204070910,00000001020201000304020004070910,00000000020201000304020104070910,01000000030100000304020104070910,00000000010101000404020104070910,00000002010101000404020104070910,01000100000101020404020104070910,00000002000102020105020104070910,00000000010100030105030104070910,00000001000002030105030104070910,01000000020300010105030104070910,01000002000203010105030104070910,01020100020301000105030104070910,00020001020302000105030104070910,01000201000003020105030104070910,00000100000002020205040104070910,00000001000001030205040104070910,00000001000001030205040104070910,01000100010300000205040104070910,00000100020301000205040104070910,01000100020301000205040104070910,02000000020301010205040104070910,00000000030301010305040104070910,01000000000301000305040204070910,00000000010301010305040204070910,00010000000103020305040204070910,00000001000103020305040204070910,00010000010103010305040304070910,01000000020103010305040304070910,00000100020103010305040304070910,01000001020103010305040304070910,00000102020103010305040304070910,01020100020103010305040304070910,01010201020103010305040304070910,01020201020103010305040304070910,01000203020103010305040304070910,01020103020103010305040304070910,01020103020103010305040304070910,01020103020103010305040304070910,;\n",
      "000001010000000000010100000000000000000000000000000100020001010101,00000000000100000100010201010101,00000001020000000200000001010200,00000201000000020000000100000202,00010201010000020000000100000003,00020201000001020000000101000003,00000000000100010000020101020103,00000000000000010001020201020103,00000000010000000103000001020103,01030301020000000100010000000000,00010000010000000200030001030101,01000000010000000203000202030200,02030202010300000300000000000001,00020303000001030000000300000001,00000000000000030102030400020101,00000000000000030000030401000202,0000\n",
      "[22, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 68, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 4]\n",
      "1\n",
      "    0    0    0    4\n",
      "    4   16    4   16\n",
      "  128  512 1024\n",
      "\n",
      "\n",
      "    0    0    0    2\n",
      "    0    2    0    0\n",
      "    4    4   16    4\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    2    0    0\n",
      "    0    2    0    2\n",
      "    4    4   16    4\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    0    4    2    2\n",
      "    4    4   16    4\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    2\n",
      "    4    4    2    0\n",
      "    8   16    4    0\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    4    4    2    0\n",
      "    8   16    4    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    0    0    0\n",
      "    8    2    0    0\n",
      "    8   16    4    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    2    2    2    0\n",
      "   16   16    4    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    4\n",
      "    2    2    2    0\n",
      "   16   16    4    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    0    2    0\n",
      "    0    2    2    4\n",
      "   16   16    4    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    4\n",
      "    0    2    4    4\n",
      "    2   32    4    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    2    2    0    8\n",
      "    2   32    8    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    2\n",
      "    0    0    4    8\n",
      "    2   32    8    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    0    0    0\n",
      "    4    8    0    2\n",
      "    2   32    8    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    0    0    4\n",
      "    0    4    8    2\n",
      "    2   32    8    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    4    2    0\n",
      "    4    8    2    0\n",
      "    2   32    8    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    4    0    2\n",
      "    4    8    4    0\n",
      "    2   32    8    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    0    4    2\n",
      "    0    0    8    4\n",
      "    2   32    8    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    2    0\n",
      "    0    0    4    4\n",
      "    4   32   16    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    2\n",
      "    0    0    2    8\n",
      "    4   32   16    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    2\n",
      "    0    0    2    8\n",
      "    4   32   16    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    0    2    0\n",
      "    2    8    0    0\n",
      "    4   32   16    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    2    0\n",
      "    4    8    2    0\n",
      "    4   32   16    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    0    2    0\n",
      "    4    8    2    0\n",
      "    4   32   16    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    4    0    0    0\n",
      "    4    8    2    2\n",
      "    4   32   16    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    8    8    2    2\n",
      "    8   32   16    2\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    0    0    0\n",
      "    0    8    2    0\n",
      "    8   32   16    4\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    2    8    2    2\n",
      "    8   32   16    4\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    2    0    0\n",
      "    0    2    8    4\n",
      "    8   32   16    4\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    2\n",
      "    0    2    8    4\n",
      "    8   32   16    4\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    2    0    0\n",
      "    2    2    8    2\n",
      "    8   32   16    8\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    0    0    0\n",
      "    4    2    8    2\n",
      "    8   32   16    8\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    2    0\n",
      "    4    2    8    2\n",
      "    8   32   16    8\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    0    0    2\n",
      "    4    2    8    2\n",
      "    8   32   16    8\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    2    4\n",
      "    4    2    8    2\n",
      "    8   32   16    8\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    4    2    0\n",
      "    4    2    8    2\n",
      "    8   32   16    8\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    2    4    2\n",
      "    4    2    8    2\n",
      "    8   32   16    8\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    4    4    2\n",
      "    4    2    8    2\n",
      "    8   32   16    8\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    0    4    8\n",
      "    4    2    8    2\n",
      "    8   32   16    8\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    4    2    8\n",
      "    4    2    8    2\n",
      "    8   32   16    8\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    4    2    8\n",
      "    4    2    8    2\n",
      "    8   32   16    8\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    2    4    2    8\n",
      "    4    2    8    2\n",
      "    8   32   16    8\n",
      "   16  128  512 1024\n",
      "\n",
      "\n",
      "\n",
      "    0    0    2    2\n",
      "    0    0    0    0\n",
      "    0    2    2    0\n",
      "    0    0    0    0\n",
      "    0    0    0    0\n",
      "    0    0    0    0\n",
      "    0    2    0    4\n",
      "    0    2    2    2\n",
      "    2\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    0    2    0    0\n",
      "    2    0    2    4\n",
      "    2    2    2    2\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    2\n",
      "    4    0    0    0\n",
      "    4    0    0    0\n",
      "    2    2    4    0\n",
      "\n",
      "\n",
      "\n",
      "    0    0    4    2\n",
      "    0    0    0    4\n",
      "    0    0    0    2\n",
      "    0    0    4    4\n",
      "\n",
      "\n",
      "\n",
      "    0    2    4    2\n",
      "    2    0    0    4\n",
      "    0    0    0    2\n",
      "    0    0    0    8\n",
      "\n",
      "\n",
      "\n",
      "    0    4    4    2\n",
      "    0    0    2    4\n",
      "    0    0    0    2\n",
      "    2    0    0    8\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    0    2    0    2\n",
      "    0    0    4    2\n",
      "    2    4    2    8\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    0    0    0    2\n",
      "    0    2    4    4\n",
      "    2    4    2    8\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    2    0    0    0\n",
      "    2    8    0    0\n",
      "    2    4    2    8\n",
      "\n",
      "\n",
      "\n",
      "    2    8    8    2\n",
      "    4    0    0    0\n",
      "    2    0    2    0\n",
      "    0    0    0    0\n",
      "\n",
      "\n",
      "\n",
      "    0    2    0    0\n",
      "    2    0    0    0\n",
      "    4    0    8    0\n",
      "    2    8    2    2\n",
      "\n",
      "\n",
      "\n",
      "    2    0    0    0\n",
      "    2    0    0    0\n",
      "    4    8    0    4\n",
      "    4    8    4    0\n",
      "\n",
      "\n",
      "\n",
      "    4    8    4    4\n",
      "    2    8    0    0\n",
      "    8    0    0    0\n",
      "    0    0    0    2\n",
      "\n",
      "\n",
      "\n",
      "    0    4    8    8\n",
      "    0    0    2    8\n",
      "    0    0    0    8\n",
      "    0    0    0    2\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    0    0    0    8\n",
      "    2    4    8   16\n",
      "    0    4    2    2\n",
      "\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    0    0    0    8\n",
      "    0    0    8   16\n",
      "    2    0    4    4\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_digits(n):\n",
    "    count = 1\n",
    "    while n // 10 >= 1:\n",
    "        n /= 10 \n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def print_states(s):\n",
    "    for i, game in enumerate(s.split(f';\\n')): \n",
    "        for state in game.split(',')[:-1]:\n",
    "            enc_state = [2**int(''.join(state[i:i+2])) for i in range(0, len(state), 2)]\n",
    "            for i, tile in enumerate(enc_state):\n",
    "                print(' ' * max(0, 5 - count_digits(tile)) + (str(tile) if tile != 1 else '0'), end=('\\n' if (i+1) % 4 == 0 else ''))\n",
    "\n",
    "            print('\\n\\n')\n",
    "res = decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=1000)[0].tolist())\n",
    "print(res)\n",
    "print([len(state) for state in res.split(',')])\n",
    "print(count_digits(0))\n",
    "print_states(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ggpa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
