{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5819fc30",
   "metadata": {},
   "source": [
    "### GPT for Learning how to play 2048\n",
    "\n",
    "Inspired by [GPT from scratch](https://www.youtube.com/watch?v=kCc8FmEb1nY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1c2d08",
   "metadata": {},
   "source": [
    "#### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee4548fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000100000002040100020000010000;R;01000001000002040000010200000001\n",
      "00000000000001040201000000000202;U;02010104000002020001000000000000\n",
      "00000001000000010003010000000101;R;01000001000000010000030100000002\n",
      "00000000010000000100000201000106;R;00000000000001010000010200000206\n",
      "00010000040100010001000300010001;L;01000000040200000103000002000100\n",
      "00020100000101000101010304000401;U;01020203040201010000040000010000\n",
      "00000101000001020100030400030101;U;01030201000003020000010400000101\n",
      "01020003050000000500000002010201;R;00010203010000050000000502010201\n",
      "00020000010400000103020000040400;D;00020100000400000003020002040400\n",
      "00000001000000000000000100020000;U;00020002020000000000000000000000\n",
      "04010709040203000000000002000002;U;05010709020203020000000000020000\n",
      "00000201080000030001050101000400;L;02010001080300000105010001040000\n",
      "00010200000106000000000400020000;R;00000102000001060100000400000002\n",
      "01010000000100000400000301000200;D;00000000010100000400000001020203\n",
      "00010201000000020100040001000002;D;0200000000000\n",
      "6800000\n"
     ]
    }
   ],
   "source": [
    "with open('2048_move_data.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:1000])\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f035a56",
   "metadata": {},
   "source": [
    "#### Tokenize\n",
    "\n",
    "Each tile is represented by 2 digit, 0-led int. Will convert tile into regular int. \n",
    "\n",
    "Each state is separated by comma, each game by ';\\n'. Will convert these into int as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c614918a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 0, 0, 0, 0, 0, 3, 1, 2, 2, 0, 0, 1, 18, 21, 18, 0, 0, 1, 2, 1, 0, 0, 0, 0, 3, 1, 2, 0, 0, 2, 1, 19, 6, 0, 2, 0, 1, 0, 6, 0, 0, 3, 0, 0, 1, 0, 0, 5, 18, 21, 18, 1, 0, 6, 2, 0, 0, 1, 6, 0, 0, 0, 3, 0, 0, 1, 5]\n",
      "01010001000000000003010202000001;R;00000102010000000003010200000201\n",
      "06000200010006000003000001000005;R;01000602000001060000000300000105\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "vocab = ['0'+str(i) for i in range(10)] + [str(i) for i in range(10, 18)]  + [';','\\n','L','R','U','D']\n",
    "mapping = {k: v for v, k in enumerate(vocab)}\n",
    "inv_mapping = {v: k for k, v in mapping.items()}\n",
    "\n",
    "def encode(s:str)-> list[str]:\n",
    "    out = []\n",
    "    pattern = r'\\d{2}|[\\n;RLUD]'\n",
    "    out = re.findall(pattern, s)\n",
    "    out = list(map(lambda k: mapping[k], out))\n",
    "    return out\n",
    "\n",
    "def decode(l:list[int]) -> str:\n",
    "    s = ''.join(list(map(lambda v: str(inv_mapping[v]), l)))\n",
    "    return s        \n",
    "\n",
    "\n",
    "print(encode('01010001000000000003010202000001;R;00000102010000000003010200000201\\n06000200010006000003000001000005;R;01000602000001060000000300000105'))\n",
    "print(decode(encode('01010001000000000003010202000001;R;00000102010000000003010200000201\\n06000200010006000003000001000005;R;01000602000001060000000300000105')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41638313",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "\n",
    "Get train test split\n",
    "\n",
    "Set up batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f67652d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "2880000 3600000 2880000 720000\n",
      "16\n",
      "tensor([[ 3,  0,  0,  ..., 18, 22, 18],\n",
      "        [ 0,  1,  2,  ..., 18, 20, 18],\n",
      "        [ 0,  0,  0,  ..., 18, 20, 18],\n",
      "        ...,\n",
      "        [ 0,  3,  1,  ..., 18, 21, 18],\n",
      "        [ 0,  1,  0,  ..., 18, 21, 18],\n",
      "        [ 1,  2,  0,  ..., 18, 23, 18]])\n",
      "tensor([[3, 2, 2,  ..., 0, 1, 0],\n",
      "        [1, 2, 0,  ..., 3, 0, 0],\n",
      "        [4, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 3,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 2, 1]])\n",
      "tensor([[ 2,  2,  4,  ...,  1,  0, 19],\n",
      "        [ 2,  0,  0,  ...,  0,  0, 19],\n",
      "        [ 0,  0,  0,  ...,  0,  0, 19],\n",
      "        ...,\n",
      "        [ 0,  3,  1,  ...,  0,  0, 19],\n",
      "        [ 0,  0,  1,  ...,  1,  1, 19],\n",
      "        [ 0,  0,  0,  ...,  2,  1, 19]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1748)\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "vocab_size = len(vocab)\n",
    "batch_size = 64\n",
    "line_size = 36\n",
    "x_size = 19 # two boards * n\n",
    "y_size = 17\n",
    "device = 'cpu' if torch.cuda.is_available() else 'cpu'\n",
    "num_iters = 5000\n",
    "eval_iters = 200\n",
    "eval_interval = num_iters//10\n",
    "n_embed = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout= 0.1\n",
    "learning_rate = 3e-4\n",
    "\n",
    "print(device)\n",
    "\n",
    "# get first game in last 20% of data\n",
    "n = int(0.8*len(data))\n",
    "while n % line_size != 0:\n",
    "    n += 1\n",
    "\n",
    "train_data = data[:n]\n",
    "test_data = data[n:]\n",
    "print(n, len(data), len(train_data), len(test_data))\n",
    "\n",
    "def get_batch(split:bool=0)-> list[torch.Tensor]:\n",
    "    # split == 0: train, 1: test\n",
    "    data = train_data if split == 0 else test_data\n",
    "    ix = torch.randint(len(data)//line_size, (batch_size,))\n",
    "    print((46*line_size+line_size-1) - (46*line_size+x_size))\n",
    "    x = torch.stack([data[i*line_size:i*line_size+x_size] for i in ix])\n",
    "    y = torch.stack([data[i*line_size+x_size:i*line_size+line_size-1] for i in ix])\n",
    "    target = torch.stack([data[i*line_size+x_size+1:i*line_size+line_size] for i in ix])\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    target = target.to(device)\n",
    "    return x, y, target\n",
    "\n",
    "x, y, target = get_batch()\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(y)\n",
    "\n",
    "print(target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d6662",
   "metadata": {},
   "source": [
    "#### Bigram Model\n",
    "\n",
    "Loss With multi-self attention: 4900: Train loss: 1.2557, Val loss: 1.1633\n",
    "\n",
    "Residual Blocks: 4900: Train loss: 1.2156, Val loss: 1.1171\n",
    "Seems to have learned state size\n",
    "\n",
    "Scaling up model 4500: Train loss: 0.4378, Val loss: 0.4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e74866",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 214\u001b[39m\n\u001b[32m    212\u001b[39m model = DecoderTransformer()\n\u001b[32m    213\u001b[39m m = model.to(device)\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m logits, loss = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mprint\u001b[39m(logits.shape)\n\u001b[32m    216\u001b[39m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ggpa-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ggpa-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 186\u001b[39m, in \u001b[36mDecoderTransformer.forward\u001b[39m\u001b[34m(self, idx, idy, targets)\u001b[39m\n\u001b[32m    184\u001b[39m y = tok_emb + pos_emb\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     y = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    187\u001b[39m y = \u001b[38;5;28mself\u001b[39m.ln_f(y)\n\u001b[32m    188\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(y) \u001b[38;5;66;03m# (B,T,vocab_size)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ggpa-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ggpa-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 126\u001b[39m, in \u001b[36mDecoderBlock.forward\u001b[39m\u001b[34m(self, x, K, V)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, K, V):\n\u001b[32m    125\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.msa(\u001b[38;5;28mself\u001b[39m.ln1(x))\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.ffwd(\u001b[38;5;28mself\u001b[39m.ln3(x))\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ggpa-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ggpa-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x, K, V)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, K=\u001b[38;5;28;01mNone\u001b[39;00m, V=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     out = torch.cat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.heads], dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     95\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28mself\u001b[39m.proj(out))\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ggpa-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ggpa-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mHead.forward\u001b[39m\u001b[34m(self, x, K, V)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, K=\u001b[38;5;28;01mNone\u001b[39;00m, V=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m K:\n\u001b[32m     54\u001b[39m         k = \u001b[38;5;28mself\u001b[39m.key(x)\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1748)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class MaskedHead(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, dim):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(dim, dim)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "    \n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size, dim):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(dim, dim)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, K=None, V=None):\n",
    "        if K is None:\n",
    "            k = self.key(x)\n",
    "        else:\n",
    "            k = self.key(K)\n",
    "        if V is None:\n",
    "            v = self.value(x)\n",
    "        else:\n",
    "            v = self.value(V)\n",
    "\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x)\n",
    "        \n",
    "\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size, dim):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([MaskedHead(head_size, dim) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size, dim):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, dim) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, K=None, V=None):\n",
    "        out = torch.cat([h(x, K, V) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4* n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head, dim):\n",
    "        super().__init__()\n",
    "        head_size = n_embed//n_head\n",
    "        self.msa = MaskedMultiHeadAttention(n_head, head_size, dim)\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, dim)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        self.ln3 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x, K, V):\n",
    "        x = x + self.msa(self.ln1(x))\n",
    "        x = x + self.sa(self.ln2(x), K, V)\n",
    "        x = x + self.ffwd(self.ln3(x))\n",
    "        return x\n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, n_head, dim):\n",
    "        super().__init__()\n",
    "        head_size = n_embed//n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size, dim)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class EncoderTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(x_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[EncoderBlock(n_embed, n_head, x_size) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T, C)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size=vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(y_size, n_embed)\n",
    "        self.blocks = [DecoderBlock(n_embed, n_head, y_size) for _ in range(n_layer)]\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        self.encoder = EncoderTransformer(vocab_size=vocab_size)\n",
    "\n",
    "    def forward(self, idx, idy=None, targets=None ):\n",
    "        Bx, Tx = idx.shape\n",
    "        By, Ty = idy.shape\n",
    "\n",
    "        x = self.encoder(idx)\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idy) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(Ty, device=device)) # (T, C)\n",
    "        y = tok_emb + pos_emb\n",
    "        for block in self.blocks:\n",
    "            y = block(y, x, x)\n",
    "        y = self.ln_f(y)\n",
    "        logits = self.lm_head(y) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idy, max_new_tokens):\n",
    "        # idx is (B,T)\n",
    "        for  _ in range(max_new_tokens):\n",
    "            idy_cond = idy[:, -y_size:]\n",
    "            logits, loss = self(idy_cond)\n",
    "            logits = logits[:,-1,:] # last time step, (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idy_next = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idy = torch.cat((idy, idy_next), dim=1) #(B, T+1)\n",
    "        return idy\n",
    "\n",
    "    \n",
    "model = DecoderTransformer()\n",
    "m = model.to(device)\n",
    "logits, loss = m(x,y, target)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd87befe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Train loss: 3.5261, Val loss: 3.5255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m     losses = estimate_loss()\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Val loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m xb, yb = \u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m logits, loss = m(xb, yb)\n\u001b[32m     28\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mget_batch\u001b[39m\u001b[34m(split)\u001b[39m\n\u001b[32m     33\u001b[39m x = torch.stack([data[i:i+block_size] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[32m     34\u001b[39m y = torch.stack([data[i+\u001b[32m1\u001b[39m:i+block_size+\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ix])\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m x = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m y = y.to(device)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [0, 1]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)\n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "for iter in range(num_iters):\n",
    "\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f'{iter}: Train loss: {losses[0]:.4f}, Val loss: {losses[1]:.4f}')\n",
    "    xb, yb = get_batch(0)\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "res = decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=100)[0].tolist())\n",
    "print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16434495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000003010306\n",
      "01000102000106010201000002010200;D;02000000010001000101060203020201\n",
      "02010101000000050002010000020103;D;00010000000000010001010502030203\n",
      "00010000040100000002010004000200;L;010001\n",
      "[190]\n"
     ]
    }
   ],
   "source": [
    "res = decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=100)[0].tolist())\n",
    "print(res)\n",
    "print([len(state) for state in res.split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991c5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000\n",
      "01000001010000010103000001030003;L;02000000020001000103000001040000\n",
      "04000000000601040006000000000002;D;01000000000000000406000405060102\n",
      "03000300010000080000010200000001;L;04000000010801000102000001000000\n",
      "04010700010000000701000001000000;D;00000000040000010100000007020700\n",
      "02000100000106000200000003010003;D;00010000000000000300010003020603\n",
      "02010202000000060001000100010604;D;00010000000000020001020602020604\n",
      "03010100040001020101020400000104;D;00000000030002020401020201020105\n",
      "00000000000203000100000500000002;R;00000000000002030100010500000002\n",
      "00010200010200000201010001000000;U;01010200030201000101000000010000\n",
      "02000100010001010100030004000005;U;02010201010003050400000002000000\n",
      "00010100000302010102000200000100;D;00000000000101000003020101020102\n",
      "01000401020100000002000100010001;D;00000000000100000102040102010002\n",
      "01000002000200010002000000010001;U;01030002000100010000000000000001\n",
      "00030000010000000500030100000002;U;01030301050000020000000000000001\n",
      "01010103040001020200000100020100;L;02010300040102010301000002010100\n",
      "01000100000105010000000101000002;R;00000002000105010001000100000102\n",
      "00000200070000000004000002020402;R;00010002000000070000000400030402\n",
      "01000106000500000003000201010000;U;02050106000300020001010000000000\n",
      "00010001000201020100010000010001;L;02000001020102000200000002000000\n",
      "04010000000002000401000100040200;R;00000401000000020000040200010402\n",
      "00000000000106010100010001000100;L;00000001010601000200000002000000\n",
      "00000000070001000000000000000001;R;00000000000007010000000000000001\n",
      "01000206000200000201010302030100;U;01020206030102030003000100000000\n",
      "01000304010300010106010000000000;U;02030304020601010000000000010000\n",
      "00000401000100000100000002010003;U;01020401020000030000000000000100\n",
      "02000000000000010000010101000002;L;02000000010100000200000001020000\n",
      "00030001020000000001030103010100;L;03010000020001000203010003020000\n",
      "01020000000000040004010100000000;L;01020000040000\n",
      "[1890]\n",
      "1\n",
      "    0    0\n",
      "\n",
      "    2    0    0    2\n",
      "    2    0    0    2\n",
      "    2    8    0    0\n",
      "    2    8    0    8\n",
      "\n",
      "\n",
      "L\n",
      "\n",
      "\n",
      "    4    0    0    0\n",
      "    4    0    2    0\n",
      "    2    8    0    0\n",
      "    2   16    0    0\n",
      "\n",
      "\n",
      "   16    0    0    0\n",
      "    0   64    2   16\n",
      "    0   64    0    0\n",
      "    0    0    0    4\n",
      "\n",
      "\n",
      "D\n",
      "\n",
      "\n",
      "    2    0    0    0\n",
      "    0    0    0    0\n",
      "   16   64    0   16\n",
      "   32   64    2    4\n",
      "\n",
      "\n",
      "    8    0    8    0\n",
      "    2    0    0  256\n",
      "    0    0    2    4\n",
      "    0    0    0    2\n",
      "\n",
      "\n",
      "L\n",
      "\n",
      "\n",
      "   16    0    0    0\n",
      "    2  256    2    0\n",
      "    2    4    0    0\n",
      "    2    0    0    0\n",
      "\n",
      "\n",
      "   16    2  128    0\n",
      "    2    0    0    0\n",
      "  128    2    0    0\n",
      "    2    0    0    0\n",
      "\n",
      "\n",
      "D\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "   16    0    0    2\n",
      "    2    0    0    0\n",
      "  128    4  128    0\n",
      "\n",
      "\n",
      "    4    0    2    0\n",
      "    0    2   64    0\n",
      "    4    0    0    0\n",
      "    8    2    0    8\n",
      "\n",
      "\n",
      "D\n",
      "\n",
      "\n",
      "    0    2    0    0\n",
      "    0    0    0    0\n",
      "    8    0    2    0\n",
      "    8    4   64    8\n",
      "\n",
      "\n",
      "    4    2    4    4\n",
      "    0    0    0   64\n",
      "    0    2    0    2\n",
      "    0    2   64   16\n",
      "\n",
      "\n",
      "D\n",
      "\n",
      "\n",
      "    0    2    0    0\n",
      "    0    0    0    4\n",
      "    0    2    4   64\n",
      "    4    4   64   16\n",
      "\n",
      "\n",
      "    8    2    2    0\n",
      "   16    0    2    4\n",
      "    2    2    4   16\n",
      "    0    0    2   16\n",
      "\n",
      "\n",
      "D\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    8    0    4    4\n",
      "   16    2    4    4\n",
      "    2    4    2   32\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    0    4    8    0\n",
      "    2    0    0   32\n",
      "    0    0    0    4\n",
      "\n",
      "\n",
      "R\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    0    0    4    8\n",
      "    2    0    2   32\n",
      "    0    0    0    4\n",
      "\n",
      "\n",
      "    0    2    4    0\n",
      "    2    4    0    0\n",
      "    4    2    2    0\n",
      "    2    0    0    0\n",
      "\n",
      "\n",
      "U\n",
      "\n",
      "\n",
      "    2    2    4    0\n",
      "    8    4    2    0\n",
      "    2    2    0    0\n",
      "    0    2    0    0\n",
      "\n",
      "\n",
      "    4    0    2    0\n",
      "    2    0    2    2\n",
      "    2    0    8    0\n",
      "   16    0    0   32\n",
      "\n",
      "\n",
      "U\n",
      "\n",
      "\n",
      "    4    2    4    2\n",
      "    2    0    8   32\n",
      "   16    0    0    0\n",
      "    4    0    0    0\n",
      "\n",
      "\n",
      "    0    2    2    0\n",
      "    0    8    4    2\n",
      "    2    4    0    4\n",
      "    0    0    2    0\n",
      "\n",
      "\n",
      "D\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    0    2    2    0\n",
      "    0    8    4    2\n",
      "    2    4    2    4\n",
      "\n",
      "\n",
      "    2    0   16    2\n",
      "    4    2    0    0\n",
      "    0    4    0    2\n",
      "    0    2    0    2\n",
      "\n",
      "\n",
      "D\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    0    2    0    0\n",
      "    2    4   16    2\n",
      "    4    2    0    4\n",
      "\n",
      "\n",
      "    2    0    0    4\n",
      "    0    4    0    2\n",
      "    0    4    0    0\n",
      "    0    2    0    2\n",
      "\n",
      "\n",
      "U\n",
      "\n",
      "\n",
      "    2    8    0    4\n",
      "    0    2    0    2\n",
      "    0    0    0    0\n",
      "    0    0    0    2\n",
      "\n",
      "\n",
      "    0    8    0    0\n",
      "    2    0    0    0\n",
      "   32    0    8    2\n",
      "    0    0    0    4\n",
      "\n",
      "\n",
      "U\n",
      "\n",
      "\n",
      "    2    8    8    2\n",
      "   32    0    0    4\n",
      "    0    0    0    0\n",
      "    0    0    0    2\n",
      "\n",
      "\n",
      "    2    2    2    8\n",
      "   16    0    2    4\n",
      "    4    0    0    2\n",
      "    0    4    2    0\n",
      "\n",
      "\n",
      "L\n",
      "\n",
      "\n",
      "    4    2    8    0\n",
      "   16    2    4    2\n",
      "    8    2    0    0\n",
      "    4    2    2    0\n",
      "\n",
      "\n",
      "    2    0    2    0\n",
      "    0    2   32    2\n",
      "    0    0    0    2\n",
      "    2    0    0    4\n",
      "\n",
      "\n",
      "R\n",
      "\n",
      "\n",
      "    0    0    0    4\n",
      "    0    2   32    2\n",
      "    0    2    0    2\n",
      "    0    0    2    4\n",
      "\n",
      "\n",
      "    0    0    4    0\n",
      "  128    0    0    0\n",
      "    0   16    0    0\n",
      "    4    4   16    4\n",
      "\n",
      "\n",
      "R\n",
      "\n",
      "\n",
      "    0    2    0    4\n",
      "    0    0    0  128\n",
      "    0    0    0   16\n",
      "    0    8   16    4\n",
      "\n",
      "\n",
      "    2    0    2   64\n",
      "    0   32    0    0\n",
      "    0    8    0    4\n",
      "    2    2    0    0\n",
      "\n",
      "\n",
      "U\n",
      "\n",
      "\n",
      "    4   32    2   64\n",
      "    0    8    0    4\n",
      "    0    2    2    0\n",
      "    0    0    0    0\n",
      "\n",
      "\n",
      "    0    2    0    2\n",
      "    0    4    2    4\n",
      "    2    0    2    0\n",
      "    0    2    0    2\n",
      "\n",
      "\n",
      "L\n",
      "\n",
      "\n",
      "    4    0    0    2\n",
      "    4    2    4    0\n",
      "    4    0    0    0\n",
      "    4    0    0    0\n",
      "\n",
      "\n",
      "   16    2    0    0\n",
      "    0    0    4    0\n",
      "   16    2    0    2\n",
      "    0   16    4    0\n",
      "\n",
      "\n",
      "R\n",
      "\n",
      "\n",
      "    0    0   16    2\n",
      "    0    0    0    4\n",
      "    0    0   16    4\n",
      "    0    2   16    4\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    0    2   64    2\n",
      "    2    0    2    0\n",
      "    2    0    2    0\n",
      "\n",
      "\n",
      "L\n",
      "\n",
      "\n",
      "    0    0    0    2\n",
      "    2   64    2    0\n",
      "    4    0    0    0\n",
      "    4    0    0    0\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "  128    0    2    0\n",
      "    0    0    0    0\n",
      "    0    0    0    2\n",
      "\n",
      "\n",
      "R\n",
      "\n",
      "\n",
      "    0    0    0    0\n",
      "    0    0  128    2\n",
      "    0    0    0    0\n",
      "    0    0    0    2\n",
      "\n",
      "\n",
      "    2    0    4   64\n",
      "    0    4    0    0\n",
      "    4    2    2    8\n",
      "    4    8    2    0\n",
      "\n",
      "\n",
      "U\n",
      "\n",
      "\n",
      "    2    4    4   64\n",
      "    8    2    4    8\n",
      "    0    8    0    2\n",
      "    0    0    0    0\n",
      "\n",
      "\n",
      "    2    0    8   16\n",
      "    2    8    0    2\n",
      "    2   64    2    0\n",
      "    0    0    0    0\n",
      "\n",
      "\n",
      "U\n",
      "\n",
      "\n",
      "    4    8    8   16\n",
      "    4   64    2    2\n",
      "    0    0    0    0\n",
      "    0    2    0    0\n",
      "\n",
      "\n",
      "    0    0   16    2\n",
      "    0    2    0    0\n",
      "    2    0    0    0\n",
      "    4    2    0    8\n",
      "\n",
      "\n",
      "U\n",
      "\n",
      "\n",
      "    2    4   16    2\n",
      "    4    0    0    8\n",
      "    0    0    0    0\n",
      "    0    0    2    0\n",
      "\n",
      "\n",
      "    4    0    0    0\n",
      "    0    0    0    2\n",
      "    0    0    2    2\n",
      "    2    0    0    4\n",
      "\n",
      "\n",
      "L\n",
      "\n",
      "\n",
      "    4    0    0    0\n",
      "    2    2    0    0\n",
      "    4    0    0    0\n",
      "    2    4    0    0\n",
      "\n",
      "\n",
      "    0    8    0    2\n",
      "    4    0    0    0\n",
      "    0    2    8    2\n",
      "    8    2    2    0\n",
      "\n",
      "\n",
      "L\n",
      "\n",
      "\n",
      "    8    2    0    0\n",
      "    4    0    2    0\n",
      "    4    8    2    0\n",
      "    8    4    0    0\n",
      "\n",
      "\n",
      "    2    4    0    0\n",
      "    0    0    0   16\n",
      "    0   16    2    2\n",
      "    0    0    0    0\n",
      "\n",
      "\n",
      "L\n",
      "\n",
      "\n",
      "    2    4    0    0\n",
      "   16    0    0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_digits(n):\n",
    "    count = 1\n",
    "    while n // 10 >= 1:\n",
    "        n /= 10 \n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def print_states(s):\n",
    "    for i, game in enumerate(s.split(f'\\n')): \n",
    "        for state in game.split(';'):\n",
    "            if len(state) == 1:\n",
    "                print(state)\n",
    "            else:\n",
    "                enc_state = [2**int(''.join(state[i:i+2])) for i in range(0, len(state), 2)]\n",
    "                for i, tile in enumerate(enc_state):\n",
    "                    print(' ' * max(0, 5 - count_digits(tile)) + (str(tile) if tile != 1 else '0'), end=('\\n' if (i+1) % 4 == 0 else ''))\n",
    "\n",
    "            print('\\n')\n",
    "res = decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=1000)[0].tolist())\n",
    "print(res)\n",
    "print([len(state) for state in res.split(',')])\n",
    "print(count_digits(0))\n",
    "print_states(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ggpa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
